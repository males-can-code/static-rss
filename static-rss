#!/usr/bin/python3

import os,sys
import feedparser
import hashlib
import sqlite3
import shutil
import subprocess
from lib.log import Log
from time import strftime
from string import Template
from config import Config
from bs4 import BeautifulSoup

class GenerateHTML(object):
    def copy_file(self, src, dest):
    # Copy a file
        try:
            shutil.copy(src, dest)
            return True
        except IOError as e:
            self.log.error('Failed to copy file: %s. Quitting...'%src)
            sys.exit()


    def remove_dir(self, path):
    # TODO This needs work, the dir is not removed
    # Remove a tree
        if os.path.exists(path):
            try:
                shutil.rmtree(path)
            except:
                pass


    def check_dir(self, path):
    # Create dir if it doesn't exist, otherwise do nothing
        if not os.path.exists(path):
            try:
                os.makedirs(path)
                return True
            except IOError as e:
                self.log.error('Failed to create dir: %s. Quitting...'%path)
                sys.exit()
        else:
            return True


    def check_file(self, filename):
    # Check if file exists
        try:
            with open(filename) as f: pass
            return True
        except IOError as e:
            self.log.info('Failed to open file: %s'%filename)
            return False
        

    def get_file(self, filename):
    # Get file contents
        contents = []
        if self.check_file(filename):
            try:
                f = open(filename, 'r')
            except IOError as e:
                self.log.info('Failed to open file: %s'%filename)
                return False

            for line in f:
                #contents.append(self.sanitize(line))
                contents.append(line.rstrip())
            f.close()
            return contents
        else:
            return False
            

    def write_to_file(self, path, data):
    # (over)write data to file
        try:
            with open(path, 'w') as f:
                f.write(data)
        except IOError:
            self.log.error('Failed to write to file: %s. Quitting...'%path)
            sys.exit()


    def parse_template(self, template_path, data):
    # Parse a template file, input is a dict: {data to be replaced : data to replace with}
    # Return is filled_template containing all the replaced lines
        filled_template = ''
        template = self.get_file(template_path)
        loop_func = False
        switch = False

        if template:
            for line in template:
                # If '{{loop=' is encoutered in template keep on looping till {{endloop}} and 
                # send the lines to the function with the name 'loop_' + the stuff after '=' in template
                # Whatever returns from the loop function is added to the filled_template variable
                if '{{loop=' in line:
                    loop_lines = []
                    x,loop_func = line.split('=')
                    loop_func = loop_func.strip('}}')

                elif '{{endloop}}' in line:
                    if loop_func and len(loop_lines) > 0:
                        lines = getattr(self, 'loop_' + loop_func)(loop_lines, data)
                        filled_template = filled_template + lines
                    loop_func = False

                elif loop_func:
                    loop_lines.append(line)

                # If {{switch= is found the leading lines till {{endswitch}} are written to switch_lines list
                # At {{endswitch}} it will look for a key in self.switch that corresponds with the value after {{switch=
                # Value can be True or False
                # For now it is not possible to combine or nest switch and loop, maybe i'll change this but atm i don't need it
                elif '{{switch=' in line:
                    switch_lines = []
                    x,switch = line.split('=')
                    switch = switch.strip('}}')

                elif '{{endswitch}}' in line:
                    if switch and len(switch_lines) > 0:
                        if self.config.switch[switch]:
                            for switch_line in switch_lines:
                                for key in data.keys():
                                    if '$' + key in switch_line:
                                        switch_line = Template(switch_line).safe_substitute({key:data[key]})
                                filled_template = filled_template + switch_line + '\n'
                    switch = False

                elif switch:
                    switch_lines.append(line)

                else:
                    for key in data.keys():
                        if '$' + key in line:
                            line = Template(line).safe_substitute({key:data[key]})
                    filled_template = filled_template + line + '\n'
        return filled_template


    def loop_feed_list(self, lines, data):
    # Generate the left sidebar links
        filled_template = ''
        feed_url = {}
        last_group = ''
        # We really don't wanna generate this list for every page since this doesn't change
        # if self.feed_list is already set just return the old value
        if self.feed_list:
            return self.feed_list
        else:
            for group in self.get_groups():
                feeds = self.get_rows_by_value('feeds', 'feed_group',  group, order_by='feed_date_entered')
                for feed in feeds:
                    data['group_title'] = group

                    # Count unread entries per feed
                    entries = self.get_rows_by_value('entries', 'entry_feed_hash', feed['feed_hash'], order_by='entry_date_published')
                    unread_counter = 0
                    for entry in entries:
                        if entry['entry_read'] == 'unread':
                            unread_counter = unread_counter + 1

                    data['feed_link'] = feed['feed_title'] 
                    data['feed_url'] = self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_1.html'

                    # Parse template
                    if unread_counter > 0:
                        data['counter'] = '[' + str(unread_counter) + ']'
                        data['feed_read'] = 'unread'
                    else:
                        data['counter'] = ''
                        data['feed_read'] = 'read'

                    # Very very very dirty hack to make the group not show up every feed link
                    lines_tmp = lines[:]
                    if last_group == data['group_title']:
                        lines_tmp.pop(0)
                    else:
                        last_group = data['group_title']

                    for line in lines_tmp:
                        for key in data.keys():
                            if '$' + key in line:
                                line = Template(line).safe_substitute({key:data[key]})
                        filled_template = filled_template + line + '\n'
            self.feed_list = filled_template
        return filled_template


    def loop_entries(self, lines, feed):
    # Generate a page with entries (posts)
        filled_template = ''
        entries = self.get_rows_by_value('entries', 'entry_feed_hash', feed['feed_hash'], order_by='entry_date_published') 
        for entry in entries:
            entry['links_target'] = self.config.links_target          # Open links in self or blank
            if entry['entry_hash'] in feed['page']:
                for line in lines:
                    for k in entry.keys():
                        if '$' + k in line:
                            line = Template(line).safe_substitute({k:entry[k]})
                    filled_template = filled_template + line + '\n'
        return filled_template


    def loop_entry_list(self, lines, feed):
    # Generate the right sidebar links
        titles = ''
        pages = self.calculate_pages(feed)
        entries = self.get_rows_by_value('entries', 'entry_feed_hash', feed['feed_hash'], order_by='entry_date_published', limit=self.config.max_entries_in_sidebar) 

        for entry in entries:
            page_count = 1
            for page in pages:
                if entry['entry_hash'] in page:
                    entry['entry_url'] = self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_' + str(page_count) + '.html' 
                page_count = page_count + 1

            for line in lines:
                for key in entry.keys():
                    if '$' + key in line:
                        line = Template(line).safe_substitute({key:entry[key]})
                titles = titles + line + '\n'
        return titles


    def loop_page_links(self, lines, feed):
    # Generate the links to the different pages with entries for a feed
        page_links = ''
        for x in range(1, len(feed['pages']) + 1):
            link = {}
            link['page_url'] = self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_' + str(x) + '.html'
            link['page'] = x

            for line in lines:
                for key in link.keys():
                    if '$' + key in line:
                        line = Template(line).safe_substitute({key:link[key]})

                page_links = page_links + line + '\n'

        return page_links


    def calculate_pages(self, feed):
    # Calculate on how many pages the entries will be spread out
        sum_chars = 0
        entry_hash_list = []
        pages = []
        c = 1

        entries = self.get_rows_by_value('entries', 'entry_feed_hash', feed['feed_hash'], order_by='entry_date_published') 
        for entry in entries:
            sum_chars = sum_chars + len(entry['entry_content'])
            entry_hash_list.append(entry['entry_hash'])
            if sum_chars >= self.config.max_chars_per_page or c == len(entries):
                pages.append(entry_hash_list)
                entry_hash_list = []
                sum_chars = 0
            c = c + 1
                
        return pages


    def generate_prev_link(self, pages, feed, cur_page):
    # Generate the link to the previous page (left from the page links)
        if cur_page > 1: 
            return self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_' + str(cur_page - 1) + '.html'
        elif cur_page == 1:
            return self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_' + str(cur_page) + '.html'


    def generate_next_link(self, pages, feed, cur_page):
    # Generate the link to the next page (right from the page links)
        if cur_page == len(pages):
            return self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_' + str(cur_page) + '.html'
        elif cur_page > 0:
            return self.config.domain + '/feeds/' + feed['feed_hash'] + '/page_' + str(cur_page + 1) + '.html'


    def gen_init(self, feeds, del_dir=True):
    # Create/remove/copy dirs and files
        if del_dir:
            self.remove_dir(self.config.path_export_html)

        self.check_dir(self.config.path_export_html)
        self.check_dir(self.config.path_export_html + '/feeds')
        self.check_dir(self.config.path_export_html + '/css')
        self.copy_file(self.config.path_css, self.config.path_export_html + '/css')
        self.copy_file(self.config.path_favicon, self.config.path_export_html)

        for feed in feeds:      # Create all feed dirs which will store the entries
            self.check_dir(self.config.path_export_html + '/feeds/' + feed['feed_hash'])

        self.check_dir(self.config.path_export_html + '/php')
        self.copy_file(self.config.path_script_update, self.config.path_export_html + '/php')
        self.copy_file(self.config.path_script_mark_read, self.config.path_export_html + '/php')
        self.copy_file(self.config.path_script_subscribe, self.config.path_export_html + '/php')
        self.copy_file(self.config.path_db_manager, self.config.path_export_html + '/php')
        self.copy_file(self.config.app_dir + '/php/logo.png', self.config.path_export_html + '/php')
        self.copy_file(self.config.app_dir + '/php/bg.gif', self.config.path_export_html + '/php')
        self.copy_file(self.config.app_dir + '/php/phpliteadmin.css', self.config.path_export_html + '/php')


    def generate_HTML(self, del_dir=True, feeds_unread='all'):
    # The main method that calls the other method in class and eventually generates the HTML
        self.log.info('Generating HTML')
        feeds = self.get_table('feeds')     # Get a list of dicts containing information about a feed

        # Sometimes we don't wanna delete the export dir but just overwrite because there will be a (short) moment 
        # where you will get a 404 when a file is missing
        if del_dir == True:
            self.gen_init(feeds, del_dir=True)                # Remove/create and copy dirs and files
        else:
            self.gen_init(feeds, del_dir=False)                # Remove/create and copy dirs and files

        # This var
        self.feed_list = ''

        template = {}                       # feed_template dict contains main content for the 'feed.html' template 
        template['auto_refresh']    = self.config.switch['auto_refresh']
        template['domain']          = self.config.domain
        template['favicon']         = self.config.domain + '/favicon.ico'
        template['css']             = self.config.domain + '/css/stylesheet.css'

        # Create html pages for every page of every feed
        for feed in feeds:
            # only generate feed if it has new messages
            if feed['feed_hash'] in feeds_unread or feeds_unread == 'all':
                self.log.info('Generating: %s'%feed['feed_title'])
                template['pages']      = self.calculate_pages(feed)  # Calculate in how many pages the entries fit
                template['feed_title'] = feed['feed_title']          # Title is same for every page in feed
                template['feed_hash']  = feed['feed_hash']

                # From here generate every page in feed seperately
                cur_page = 1
                for page in template['pages']:
                    template['page']      = page
                    template['page_next'] = self.generate_next_link(template['pages'], feed, cur_page)
                    template['page_prev'] = self.generate_prev_link(template['pages'], feed, cur_page)
                    template['go_back']   = '%s/feeds/%s/page_%s.html' %(self.config.domain, feed['feed_hash'], str(cur_page))

                    # Parse the template and write to file
                    filled_template = self.parse_template(self.config.path_template_feed, template)
                    self.write_to_file('%s/feeds/%s/page_%s.html'%(self.config.path_export_html, feed['feed_hash'], str(cur_page)), filled_template)

                    # First feed, first page will be index.html
                    if feed == feeds[0] and cur_page == 1:
                        self.write_to_file(self.config.path_export_html + '/index.html', filled_template)

                    cur_page = cur_page + 1



class Database(object):
    def create_tables(self): 
    # Create database and tables
        db = sqlite3.connect(self.config.path_db)
        db.execute("create table if not exists feeds (feed_hash         text unique  ,"
                                                     "feed_title        text         ,"
                                                     "feed_date_entered int          ,"
                                                     "feed_url          text         ,"
                                                     "feed_icon_url     text         ,"
                                                     "feed_last_updated text         ,"
                                                     "feed_group        text         )")

        db.execute("create table if not exists entries (entry_hash           text unique  ,"
                                                       "entry_content        text         ,"
                                                       "entry_url            text         ,"
                                                       "entry_title          text         ,"
                                                       "entry_read           text         ,"
                                                       "entry_date_published text         ,"
                                                       "entry_feed_hash      text         )")
        db.commit()
        db.close()


    def get_table(self, table, order_by='ROWID'):
    # Spits out the complete table into a list of dictionaries
        db = sqlite3.connect(self.config.path_db)
        rows = []
        cols = []
        table_export = []

        for row in db.execute('select * from %s order by %s'%(table,order_by)):
            rows.append(row)

        for row in db.execute('PRAGMA table_info(%s)'%table):
            cols.append(row[1])

        for row in rows:
            row_export = {}
            for x in range(0,len(cols)):
                row_export[cols[x]] = row[x]
            table_export.append(row_export)

        db.close()
        return table_export


    def get_rows_by_value(self, table, col, value, order_by, limit='all'):
    # Get a list of all rows containing a value
        db = sqlite3.connect(self.config.path_db)
        rows = []
        cols = []
        rows_export = []
        if limit == 'all':
            for row in db.execute('select * from %s where "%s" is "%s" order by "%s" desc'%(table, col, value, order_by)):
                rows.append(row)
        else:
            for row in db.execute('select * from %s where "%s" is "%s" order by "%s" desc limit %s'%(table, col, value, order_by, limit)):
                rows.append(row)

        for row in db.execute('PRAGMA table_info(%s)'%table):
            cols.append(row[1])

        for row in rows:
            row_export = {}
            for x in range(0,len(cols)):
                row_export[cols[x]] = row[x]
            rows_export.append(row_export)

        db.close()
        return rows_export


    def insert_row(self, table, row):
    # Insert a row
        db = sqlite3.connect(self.config.path_db)
        values = []
        for key in row:
            values.append(row[key])
        keys = ','.join(row.keys())
        query = 'INSERT INTO %s(%s) VALUES(%s)'%(table, keys, ','.join(['?'] * len(row)))
        try:
            db.execute(query,values)
            db.commit()
            db.close()
        except sqlite3.Error as e:
            self.log.error('Failed to insert row: %s'%e.args[0])
            db.close()


    def update_row(self, table, col1, value1, col2, value2):
    # Update one or more rows, you can also feed value2 as a list
        db = sqlite3.connect(self.config.path_db)
        if type(value2) == list:
            tmp_value2 = ''
            for v in value2:
                tmp_value2 = tmp_value2 + ',\'' + v + '\''
            value2 = tmp_value2.lstrip(',')
        else:
            value2 = '\'' + value2 + '\''
        db.execute('update %s set "%s" = "%s" where "%s" in (%s)'%(str(table), str(col1), str(value1), str(col2), str(value2)))
        db.commit()
        db.close()


    def delete_older_than(self, table, days):
        db = sqlite3.connect(self.config.path_db)
        for row in db.execute('delete from %s where date("-%s day")'%(table,days)):
            self.log.info('Deleting entry: %s'%row['entry_title'])
        try:
            db.commit()
            db.close()
        except sqlite3.Error as e:
            self.log.error('Failed to delete row: %s'%e.args[0])
            db.close()


    def check_in_table(self, table, col, value):
    # Return True/False if a value exists in table
        db = sqlite3.connect(self.config.path_db)
        for data in db.execute('select * from "%s" where "%s" = "%s"'%(str(table),str(col),str(value))):
            if data:
                db.close()
                return True
        db.close()
        return False
        


class RSS(Database, GenerateHTML):
    def __init__(self):
        self.log = Log()
        self.config = Config()


    def get_timestamp(self, human_readable=True):
        if human_readable:
            timestamp = strftime("%Y-%m-%d %H:%M:%S")
        else:
            timestamp = strftime("%Y%m%d%H%M%S")
        return timestamp


    def get_hash(self, data):
    # Clean up var and create a hash of data
        data = self.sanitize(data)
        data = self.encode(data)
        hashed = hashlib.sha224(data).hexdigest()
        return hashed


    def get_groups(self):
    # Get a list of groups that contain feeds
        group_list = []
        feeds = self.get_table('feeds')
        for feed in feeds:
            if feed['feed_group'] not in group_list:
                group_list.append(feed['feed_group'])
        return group_list


    def parse_feed(self, url):
    # Parse feed with feedparser module or return False
        self.log.info('Parsing: %s'%url)
        parsed_feed = feedparser.parse(url)
        if len(parsed_feed.feed) < 1:
            self.log.error('Failed to import feed: %s'%url)
            return False
        else:
            return parsed_feed


    def parse_feeds(self):
    # Parse all feeds, calls parse_feed()
        feeds_unread = []
        feeds = self.get_table('feeds')
        for feed in feeds:
            parsed_feed = self.parse_feed(feed['feed_url'])
            if parsed_feed:
                for entry in parsed_feed.entries:
                    if not self.check_in_table('entries', 'entry_hash', str(self.get_hash(entry['title'] + str(self.clean_HTML(entry['summary']))))):
                        self.add_entry(entry, feed['feed_url'])
                        # Add feed_hash from feeds with unread messages to feeds_unread so we know which html should be regenerated
                        if feed['feed_hash'] not in feeds_unread:
                            feeds_unread.append(feed['feed_hash'])
        return feeds_unread


    def sanitize(self, var):
    # Strip file from blanks and newlines
        var = var.strip()
        return var


    def encode(self, data):
    # Convert var to utf-8
        try:
            data = data.encode('utf-8')
            return data
        except:
            self.log.warning('Failed to encode data, data is already urf-8?')


    def add_feed(self, feed):
    # Gather information to add feed to database, feed is a list [url,group]
        feed_insert = {}
        parsed_feed = self.parse_feed(feed[0])
        if len(parsed_feed.version) != 0:
            if parsed_feed:
                feed_insert['feed_hash'] = str(self.get_hash(feed[0]))
                feed_insert['feed_date_entered'] = str(self.get_timestamp())
                feed_insert['feed_url'] = str(feed[0])
                feed_insert['feed_icon_url'] = 'None'
                feed_insert['feed_last_updated'] = 'None'
                feed_insert['feed_group'] = feed[1]

                if 'title' in parsed_feed.feed.keys():
                    feed_insert['feed_title'] = str(parsed_feed.feed['title'])
                else:
                    feed_insert['feed_title'] = str(feed[0])

                self.log.info('Adding feed: %s'%feed[0])
                self.insert_row('feeds', feed_insert)
        else:
            self.log.error('Url is not a feed')


    def clean_HTML(self, content):
        soup = BeautifulSoup(content)

        for tag in self.config.invalid_tags: 
            for match in soup.findAll(tag):
                        match.replaceWithChildren()

        for tag in soup():
            for attribute in self.config.invalid_attr:
                del tag[attribute]

        return soup


    def add_entry(self, entry, url):
    # Gather information to add entry (post) to database
        self.log.info('Inserting new entry: %s'%entry['title'])
        entry_insert = {}

        # Try to get title otherwise insert link or unknown
        try:
            entry_insert['entry_title']           = str(entry['title'])
        except:
            try:
                entry_insert['entry_title']           = str(entry['link'])
                self.log.warning('Failed to find title, using \'url\'')
            except:
                entry_insert['entry_title']           = 'unknown'
                self.log.warning('Failed to find title, using \'unknown\'')

        entry_insert['entry_content']         = str(self.clean_HTML(entry['summary']))
        entry_insert['entry_hash']            = str(self.get_hash(entry_insert['entry_title'] + entry_insert['entry_content']))
        entry_insert['entry_read']            = 'unread'
        #entry_insert['entry_date_entered']    = str(self.get_timestamp(human_readable=False))
        entry_insert['entry_feed_hash']       = str(self.get_hash(url))

        # Try to get url otherwise insert unknown
        try:
            entry_insert['entry_url']       = str(entry['link'])
        except:
            entry_insert['entry_url']       = 'unknown'
            self.log.warning('Failed to find url for entry, using \'unknown\'')

        # Try to get a date if available otherwise insert timestamp
        try:
            p = entry.published_parsed
        except:
            try:
                p = entry.updated_parsed
            except:
                try:
                    p = entry.created_parsed
                except:
                    p = False

        if p:
            # The date as returned from feedparser is in form: 2013-1-12, and must be 2013-01-12
            date = []
            for x in p:
                date.append(str(x).rjust(2,'0'))
            entry_insert['entry_date_published'] = str('%s-%s-%s %s:%s:%s'%(date[0],date[1],date[2],date[3],date[4],date[5]))
        else:
            entry_insert['entry_date_published'] = str(self.get_timestamp())
            self.log.warning('Failed to find a date, using timestamp')

        self.insert_row('entries', entry_insert)


    def mark_read(self, feed_hash):
    # Mark a feed as read
        self.log.info('Marking feed read: %s'%feed_hash)
        self.update_row('entries', 'entry_read', 'read', 'entry_feed_hash', feed_hash)


    def mark_all_read(self):
    # Mark all feeds as read, and return a list of changed feeds so they can be regenerated
        feeds_mark_read = []
        entries = self.get_rows_by_value('entries', 'entry_read', 'unread', order_by='entry_date_published')
        for entry in entries:
            feeds_mark_read.append(entry['entry_feed_hash'])
        feeds_mark_read = list(set(feeds_mark_read))
        for h in feeds_mark_read:
            self.log.info('Marking feed read: %s'%h)
        self.update_row('entries', 'entry_read', 'read', 'entry_feed_hash', feeds_mark_read)
        return feeds_mark_read


    def check_ttl(self):
        if self.config.entry_ttl:
            self.log.info('Checking TTL')
            self.delete_older_than('entries',self.config.entry_ttl)


    def init(self):
    # Create tables, input feeds from config file in table
    # TODO are there feeds? what will happen if not?
        self.check_dir(os.path.dirname(self.config.path_db))
        self.create_tables()
        self.check_ttl()

        if len(self.config.feeds) >= 1:
            for feed in self.config.feeds:
                if not self.check_in_table('feeds', 'feed_hash', str(self.get_hash(feed[0]))):
                    self.add_feed(feed)
        else:
            self.log.error('No feeds configured, Quitting...')


    def usage(self):
    # Display help
        print('static-rss - An RSS reader that outputs static HTML')
        print('Syntax: static-rss [OPTION]...')
        print()
        print('Options:')
        print('    -p, --parse-feeds              -parse feeds')
        print('    -g, --gen-html                 -generate HTML')
        print('        --mark-read=<hash>         -mark feed as read')
        print('    -M, --mark-all-read            -mark all feeds as read')
        print('        --subscribe=<url>          -subscribe to feed')
        print()
        print('    -h, --help                     -this text')
                                                                 

    def run(self):
        if len(sys.argv) <= 1:
            self.usage()
            sys.exit()


        if '-h' in sys.argv or '--help' in sys.argv:
            self.usage()
            sys.exit()

        if len(sys.argv) >= 2:
            self.init()

            for arg in sys.argv:
                if '--subscribe=' in arg:
                    x,url = arg.split('=') 
                    for arg in sys.argv:
                        if '--group=' in arg:
                            x,group = arg.split('=') 
                            if len(url) > 0 and len(group) > 0:
                                self.add_feed([url, group])
                            else:
                                self.log.error('Failed to enter url... Quitting...')
                                sys.exit()
                    
            if '--parse-feeds' in sys.argv or '-p' in sys.argv:
                # We pass a list of hashes from feeds with unread messages to generate_HTML
                feeds_unread = self.parse_feeds()
                #self.generate_HTML(feeds_unread=feeds_unread)
                self.generate_HTML(feeds_unread='all')

            if '--mark-all-read' in sys.argv or '-M' in sys.argv:
                marked_read = self.mark_all_read()
                #self.generate_HTML(feeds_unread=marked_read)
                self.generate_HTML(del_dir=False, feeds_unread='all')

            for arg in sys.argv:
                if '--mark-read=' in arg:
                    x,hash = arg.split('=') 
                    self.mark_read(hash)
                    #self.generate_HTML(feeds_unread=[hash])
                    self.generate_HTML(del_dir=False, feeds_unread=[hash])
                    self.generate_HTML(del_dir=False, feeds_unread='all')

            if '--gen-html' in sys.argv or '-g' in sys.argv:
                self.generate_HTML()
                sys.exit()

app = RSS()
app.run()
